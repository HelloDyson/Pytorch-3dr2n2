{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "import gc\n",
    "import sys\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler\n",
    "torch.backends.cudnn.benchmark=True\n",
    "\n",
    "import time\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA enable:  True\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4b262b44441e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test CUDA available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CUDA enable: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'current device: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "# test CUDA available\n",
    "print('CUDA enable: ', torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('current device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import dataset from ./lib/dataset.py\n",
    "import lib.dataset as dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename + '_latest.pth.tar')\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename + '_latest.pth.tar', filename + '_best.pth.tar')\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "BASE_LR = 0.01\n",
    "EPOCH_DECAY = 10 # number of epochs after which the Learning rate is decayed exponentially.\n",
    "DECAY_WEIGHT = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hyperparameters\n",
    "batch_size = 16\n",
    "train_val_ratio = 0.8\n",
    "\n",
    "# pre setting device and data set length\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ren_len = dataset.ren_dataset.__len__()\n",
    "vox_len = dataset.vox_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This function changes the learning rate over the training model.\n",
    "def exp_lr_scheduler(optimizer, epoch, init_lr=BASE_LR, lr_decay_epoch=EPOCH_DECAY):\n",
    "    \"\"\"Decay learning rate by a factor of DECAY_WEIGHT every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = init_lr * (DECAY_WEIGHT**(epoch // lr_decay_epoch))\n",
    "\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print('LR is set to {}'.format(lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37403 25742 27527 27895 27860  9526 42788 37721 25389  7588 12693 36383\n",
      " 25394 24941  5439 34943]\n",
      "matching: image =  tensor([37403, 25742, 27527, 27895, 27860,  9526, 42788, 37721, 25389,  7588,\n",
      "        12693, 36383, 25394, 24941,  5439, 34943]) voxel =  tensor([37403, 25742, 27527, 27895, 27860,  9526, 42788, 37721, 25389,  7588,\n",
      "        12693, 36383, 25394, 24941,  5439, 34943])\n",
      "[11714 10591 34298 20655  3099 25104 31500 42511 30269 26848 35442 12160\n",
      " 26979 39204 35912 40335]\n",
      "matching: image =  tensor([11714, 10591, 34298, 20655,  3099, 25104, 31500, 42511, 30269, 26848,\n",
      "        35442, 12160, 26979, 39204, 35912, 40335]) voxel =  tensor([11714, 10591, 34298, 20655,  3099, 25104, 31500, 42511, 30269, 26848,\n",
      "        35442, 12160, 26979, 39204, 35912, 40335])\n",
      "[16258 23812 20030 11554 30200 16067 28794 22299 33370  6314 30383  1675\n",
      "   182  6343 14102 26371]\n",
      "matching: image =  tensor([16258, 23812, 20030, 11554, 30200, 16067, 28794, 22299, 33370,  6314,\n",
      "        30383,  1675,   182,  6343, 14102, 26371]) voxel =  tensor([16258, 23812, 20030, 11554, 30200, 16067, 28794, 22299, 33370,  6314,\n",
      "        30383,  1675,   182,  6343, 14102, 26371])\n",
      "[42960 28788 39896 30638  4498 29047 39259 34600 16936 19214 33915   610\n",
      " 20194 16475 41297 17807]\n",
      "matching: image =  tensor([42960, 28788, 39896, 30638,  4498, 29047, 39259, 34600, 16936, 19214,\n",
      "        33915,   610, 20194, 16475, 41297, 17807]) voxel =  tensor([42960, 28788, 39896, 30638,  4498, 29047, 39259, 34600, 16936, 19214,\n",
      "        33915,   610, 20194, 16475, 41297, 17807])\n",
      "[ 5862 12557  8918  5760 29203 39237  6829  2008 25103 19010 43515 39716\n",
      " 20787 26330  3267 19231]\n",
      "matching: image =  tensor([ 5862, 12557,  8918,  5760, 29203, 39237,  6829,  2008, 25103, 19010,\n",
      "        43515, 39716, 20787, 26330,  3267, 19231]) voxel =  tensor([ 5862, 12557,  8918,  5760, 29203, 39237,  6829,  2008, 25103, 19010,\n",
      "        43515, 39716, 20787, 26330,  3267, 19231])\n"
     ]
    }
   ],
   "source": [
    "# start an epoch\n",
    "# slice training and validation index\n",
    "rand_idx = np.random.permutation(np.arange(min(ren_len,vox_len)))\n",
    "thr = int(train_val_ratio*len(rand_idx))\n",
    "train_idx = rand_idx[:thr]\n",
    "val_idx = rand_idx[thr:]\n",
    "\n",
    "for i in range(ren_len//batch_size):\n",
    "    idx = train_idx[i*batch_size: (i+1)*batch_size]\n",
    "    print(idx)\n",
    "    render_loader, voxel_loader = dataset.get_train_data_loaders(idx)\n",
    "    for it, (images, voxels) in enumerate(zip(render_loader, voxel_loader)):\n",
    "        inputs=Variable(images[0])\n",
    "        labels=Variable(voxels[0])\n",
    "        print(\"matching: image = \", images[1], 'voxel = ',voxels[1])\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "    # test mode\n",
    "    if i >3:\n",
    "        break\n",
    "    # test mode end\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run(p=0):\n",
    "    # Parameters\n",
    "    num_epochs = 10 #origin = 10\n",
    "    output_period = 100\n",
    "    batch_size = 64 #origin = 100\n",
    "    \n",
    "    # setup the device for running\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model = models.resnet152(pretrained=False)\n",
    "    model = resnet_18()\n",
    "    model = model.to(device)#.half()\n",
    "    \n",
    "    model.drop = nn.Dropout(p)\n",
    "    print(model.drop)\n",
    "    \n",
    "\n",
    "    train_loader, val_loader = dataset.get_data_loaders(batch_size)\n",
    "    num_train_batches = len(train_loader)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    # TODO: optimizer is currently unoptimized\n",
    "    # there's a lot of room for improvement/different optimizers\n",
    "    optimizer = optim.SGD(model.parameters(), lr=BASE_LR,momentum=0.9,weight_decay=DECAY_WEIGHT)#,weight_decay=DECAY_WEIGHT,momentum=0.9) # SGD\n",
    "    # Adam Torch：learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=BASE_LR, betas=(0.9, 0.999), eps=1e-08, weight_decay=DECAY_WEIGHT)\n",
    "\n",
    "    top1trset,top5trset = [],[]\n",
    "    top1set,top5set = [],[]\n",
    "    epoch = 1\n",
    "    while epoch <= num_epochs:\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "        top5 = AverageMeter()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        optimizer = exp_lr_scheduler(optimizer, epoch)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('Current learning rate: ' + str(param_group['lr']))\n",
    "            \n",
    "        model.train()\n",
    "\n",
    "        for batch_num, (inputs, labels) in enumerate(train_loader):\n",
    "        \n",
    "            inputs=Variable(inputs)\n",
    "            labels=Variable(labels)\n",
    "            inputs = inputs.to(device)#.half()\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "    \n",
    "            outputs = outputs.float()\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "    \n",
    "            # measure accuracy and record loss\n",
    "            prec1, prec5 = accuracy(outputs.data, labels, topk=(1, 5))\n",
    "            losses.update(loss.data.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "            top5.update(prec5.item(), inputs.size(0))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "#             model.float()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            if batch_num % output_period == 0:\n",
    "                print('[%d:%.2f] loss: %.3f' % (\n",
    "                    epoch, batch_num*1.0/num_train_batches,\n",
    "                    running_loss/output_period\n",
    "                    ))\n",
    "                running_loss = 0.0\n",
    "                top1trset.append(100-top1.avg)\n",
    "                top5trset.append(100-top5.avg)\n",
    "                print('top1 training err (%)= '+str(top1trset[-1]))\n",
    "                print('top5 training err (%)= '+str(top5trset[-1]))\n",
    "\n",
    "                gc.collect()\n",
    "                \n",
    "            \n",
    "\n",
    "        gc.collect()\n",
    "        # save after every epoch\n",
    "        torch.save(model.state_dict(), \"models/model.%d\" % epoch)\n",
    "\n",
    "        # TODO: Calculate classification error and Top-5 Error\n",
    "        # on training and validation datasets here\n",
    "        \n",
    "        val_loader,test_loader = dataset.get_val_test_loaders(32)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        top1test = AverageMeter()\n",
    "        top5test = AverageMeter()\n",
    "        for i, data in enumerate(val_loader):\n",
    "            img, labels = data\n",
    "            x = Variable(img)\n",
    "            y = Variable(labels)\n",
    "\n",
    "            x = x.to(device)#.half()\n",
    "            y = y.to(device)\n",
    "            outs = model(x)\n",
    "            #get top 5 output\n",
    "            prec1test, prec5test = accuracy(outs.data, y, topk=(1, 5))\n",
    "#             losses.update(loss.data.item(), inputs.size(0))\n",
    "            top1test.update(prec1test.item(), x.size(0))\n",
    "            top5test.update(prec5test.item(), x.size(0))\n",
    "        \n",
    "        top1set.append(100-top1test.avg)\n",
    "        top5set.append(100-top5test.avg)\n",
    "        print('top1 val err (%)= '+str(top1set[-1]))\n",
    "        print('top5 val err (%)= '+str(top5set[-1]))\n",
    "        \n",
    "        gc.collect()\n",
    "        epoch += 1\n",
    "    return top1trset,top5trset,top1set,top5set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Starting training')\n",
    "# SGD, w.o. WD, 3 fc = 35, dp = 0.5. 33\n",
    "# SGD, w.o. WD=0.001, 3 fc = 3,random center in training, dp = 0.2\n",
    "# SGD, w.o. WD=0.001, 3 fc = 3,random center in training, dp = 0.2, 35\n",
    "# SGD, w.o. WD=0.001, 3 fc = 3,random center in training, dp = 0.2, BASE_LR = 0.1\n",
    "# SGD, w.o. WD=0.001, 2 fc = 3,random center in training, dp = 0.2, BASE_LR = 0.1\n",
    "BASE_LR = 0.001\n",
    "top1trset,top5trset,top1set,top5set = run(0.2)\n",
    "print('Training terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAM, w.o. WD=0.001, 2 fc = 3, dp = 0.2, BASE_LR = 0.001\n",
    "top1trset,top5trset,top1set,top5set #~43 epoch 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAM, w.o. WD=0.001, 2 fc, dp = 0.2, BASE_LR = 0.01\n",
    "top1trset,top5trset,top1set,top5set #~43 epoch 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Starting training')\n",
    "BASE_LR = 0.01\n",
    "top1trset1,top5trset1,top1set1,top5set1 = run(0.2)\n",
    "top1trset2,top5trset2,top1set2,top5set2 = run(0.5)\n",
    "top1trset3,top5trset3,top1set3,top5set3 = run(0.8)\n",
    "print('Training terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Starting training')\n",
    "BASE_LR = 0.001\n",
    "top1trset0001,top5trset0001,top1set0001,top5set0001 = run(0)\n",
    "BASE_LR = 0.0001\n",
    "top1trset00001,top5trset00001,top1set00001,top5set00001 = run(0)\n",
    "print('Training terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import the pre-trained model\n",
    "model = resnet_18()\n",
    "model_path = 'models/trained/model.25'\n",
    "checkpoint = torch.load(model_path, map_location='cpu')\n",
    "model.load_state_dict(checkpoint)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)#.half()\n",
    "\n",
    "\n",
    "# write top 5 into result.txt\n",
    "def test_accu(batch_size):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    val_loader,test_loader = dataset.get_val_test_loaders(batch_size)\n",
    "    top1_count,top5_count = 0.0,0.0\n",
    "    print('number of test set: ' + str(len(test_loader)))\n",
    "    \n",
    "    result = open(\"result.txt\",\"w\")\n",
    "    \n",
    "#     model.eval()\n",
    "        \n",
    "#     top1test = AverageMeter()\n",
    "#     top5test = AverageMeter()\n",
    "#     for i, data in enumerate(val_loader):\n",
    "#         img, labels = data\n",
    "#         x = Variable(img)\n",
    "#         y = Variable(labels)\n",
    "\n",
    "#         x = x.to(device)#.half()\n",
    "#         y = y.to(device)\n",
    "#         outs = model(x)\n",
    "#         #get top 5 output\n",
    "#         prec1test, prec5test = accuracy(outs.data, y, topk=(1, 5))\n",
    "# #             losses.update(loss.data.item(), inputs.size(0))\n",
    "#         top1test.update(prec1test.item(), x.size(0))\n",
    "#         top5test.update(prec5test.item(), x.size(0))\n",
    "\n",
    "#     top1set.append(100-top1test.avg)\n",
    "#     top5set.append(100-top5test.avg)\n",
    "#     print('top1 val err (%)= '+str(top1set[-1]))\n",
    "#     print('top5 val err (%)= '+str(top5set[-1]))\n",
    "\n",
    "#     gc.collect()\n",
    "#     epoch += 1\n",
    "\n",
    "    model.eval()\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        img, labels = data\n",
    "        x = Variable(img)\n",
    "        y = Variable(labels)\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()#.half()\n",
    "            y = y.cuda()\n",
    "        outs = model(x)\n",
    "        _, pred1 = torch.max(outs, -1)\n",
    "        _, pred5 = torch.topk(outs, 5)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            n = int(i*batch_size+b+1)\n",
    "            result.write('test/' + str(n).zfill(8)+'.jpg '+' '.join(str(int(e)) for e in pred5[b].tolist())+'\\n')\n",
    "    \n",
    "    # overwrite the result.txt every run\n",
    "    result.truncate()\n",
    "    result.close()\n",
    "    \n",
    "# Execute\n",
    "test_accu(25)\n",
    "print('completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = args.lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "        return losses,top1,top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "shist1 = [np.array(h) for h in top1trset]\n",
    "shist2 = [np.array(h) for h in top1set]\n",
    "# shist3 = [np.array(h) for h in top5set0001]\n",
    "# shist4 = [np.array(h) for h in top5set00001]\n",
    "num_epochs=10\n",
    "\n",
    "plt.title(\"Validation vs. Training Error\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist1)),shist1,label=\"Training\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist2)),shist2,label=\"Validation\")\n",
    "# plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist3)),shist3,label=\"lr=0.001\")\n",
    "# plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist4)),shist4,label=\"lr=0.0001\")\n",
    "# plt.ylim((0,1.))\n",
    "# plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "shist1 = [np.array(h) for h in top5set001]\n",
    "shist2 = [np.array(h) for h in top5set1]\n",
    "shist3 = [np.array(h) for h in top5set2]\n",
    "shist4 = [np.array(h) for h in top5set3]\n",
    "num_epochs=10\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Dropout rate\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Error\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist1)),shist1,label=\"dropout=0\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist2)),shist2,label=\"dropout=0.2\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist3)),shist3,label=\"dropout=0.5\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist4)),shist4,label=\"dropout=0.8\")\n",
    "# plt.ylim((0,1.))\n",
    "# plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "shist1 = [np.array(h) for h in top5trset001]\n",
    "shist2 = [np.array(h) for h in top5trset1]\n",
    "shist3 = [np.array(h) for h in top5trset2]\n",
    "shist4 = [np.array(h) for h in top5trset3]\n",
    "num_epochs=10\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Dropout rate\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Training Error\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist1)),shist1,label=\"dropout=0\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist2)),shist2,label=\"dropout=0.2\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist3)),shist3,label=\"dropout=0.5\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist4)),shist4,label=\"dropout=0.8\")\n",
    "# plt.ylim((0,1.))\n",
    "# plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "shist1 = [np.array(h) for h in top5trset0]\n",
    "shist2 = [np.array(h) for h in top5trset001]\n",
    "shist3 = [np.array(h) for h in top5trset0001]\n",
    "shist4 = [np.array(h) for h in top5trset00001]\n",
    "num_epochs=10\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Learning rate\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist1)),shist1,label=\"lr=0.1\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist2)),shist2,label=\"lr=0.01\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist3)),shist3,label=\"lr=0.001\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist4)),shist4,label=\"lr=0.0001\")\n",
    "# plt.ylim((0,1.))\n",
    "# plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "range(1,10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top1trset,top5trset,top1set,top5set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        target = target.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1, top5=top5))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "\n",
    "    return losses,top1,top5\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename + '_latest.pth.tar')\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename + '_latest.pth.tar', filename + '_best.pth.tar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch36]",
   "language": "python",
   "name": "conda-env-torch36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

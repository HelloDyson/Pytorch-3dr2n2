{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "import gc\n",
    "import sys\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler\n",
    "torch.backends.cudnn.benchmark=True\n",
    "\n",
    "import time\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA enable:  True\n"
     ]
    }
   ],
   "source": [
    "# test CUDA available\n",
    "print('CUDA enable: ', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import dataset from ./lib/dataset.py\n",
    "import lib.dataset as dataset\n",
    "from models.__init__ import load_model\n",
    "from lib.config import cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "BASE_LR = 0.01\n",
    "EPOCH_DECAY = 10 # number of epochs after which the Learning rate is decayed exponentially.\n",
    "DECAY_WEIGHT = 0.001\n",
    "cfg.CONST.IMG_W = 137\n",
    "cfg.CONST.IMG_H = 137\n",
    "cfg.CONST.N_VOX = 32\n",
    "cfg.CONST.N_VIEWS = 4\n",
    "from lib.solver import Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename + '_latest.pth.tar')\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename + '_latest.pth.tar', filename + '_best.pth.tar')\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hyperparameters\n",
    "batch_size = 16\n",
    "train_val_ratio = 0.8\n",
    "\n",
    "# pre setting device and data set length\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ren_len = dataset.ren_dataset.__len__()\n",
    "vox_len = dataset.vox_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This function changes the learning rate over the training model.\n",
    "def exp_lr_scheduler(optimizer, epoch, init_lr=BASE_LR, lr_decay_epoch=EPOCH_DECAY):\n",
    "    \"\"\"Decay learning rate by a factor of DECAY_WEIGHT every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = init_lr * (DECAY_WEIGHT**(epoch // lr_decay_epoch))\n",
    "\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print('LR is set to {}'.format(lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NetClass = load_model(cfg.CONST.NETWORK_CLASS)\n",
    "# print('Network definition: \\n')\n",
    "net = NetClass()\n",
    "# print(net)\n",
    "\n",
    "# start an epoch\n",
    "# slice training and validation index\n",
    "rand_idx = np.random.permutation(np.arange(min(ren_len,vox_len)))\n",
    "thr = int(train_val_ratio*len(rand_idx))\n",
    "train_idx = rand_idx[:thr]\n",
    "val_idx = rand_idx[thr:]\n",
    "\n",
    "for i in range(ren_len//batch_size):\n",
    "    idx = train_idx[i*batch_size: (i+1)*batch_size]\n",
    "    print(idx)\n",
    "    render_loader, voxel_loader = dataset.get_train_data_loaders(idx)\n",
    "    for it, (images, voxels) in enumerate(zip(render_loader, voxel_loader)):\n",
    "        inputs=Variable(torch.cat(images[0]))\n",
    "        labels=Variable(voxels[0])\n",
    "        print(\"matching: image = \", images[1], 'voxel = ',voxels[1])\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        print(inputs.shape)\n",
    "    \n",
    "    # test mode\n",
    "    if i >3:\n",
    "        break\n",
    "    # test mode end\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    # Parameters\n",
    "    num_epochs = 10\n",
    "    output_period = 100\n",
    "    batch_size = 4\n",
    "    \n",
    "    # setup the device for running\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    NetClass = load_model(cfg.CONST.NETWORK_CLASS)\n",
    "    model = NetClass().to(device)\n",
    "    \n",
    "    embed = nn.Embedding(3, 96).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=BASE_LR,momentum=0.9,weight_decay=DECAY_WEIGHT)\n",
    "    top1trset,top5trset = [],[]\n",
    "    top1set,top5set = [],[]\n",
    "    epoch = 1\n",
    "    while epoch <= num_epochs:\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "        top5 = AverageMeter()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        optimizer = exp_lr_scheduler(optimizer, epoch)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('Current learning rate: ' + str(param_group['lr']))\n",
    "            \n",
    "        model.train()\n",
    "\n",
    "        # start an epoch\n",
    "        # slice training and validation index\n",
    "        rand_idx = np.random.permutation(np.arange(min(ren_len,vox_len)))\n",
    "        thr = int(train_val_ratio*len(rand_idx))\n",
    "        train_idx = rand_idx[:thr]\n",
    "        val_idx = rand_idx[thr:]\n",
    "\n",
    "        for i in range(thr//batch_size):\n",
    "            idx = train_idx[i*batch_size: (i+1)*batch_size]\n",
    "            print(idx)\n",
    "            render_loader, voxel_loader = dataset.get_train_data_loaders(idx)\n",
    "            \n",
    "#             print('render_loader',render_loader.shape)\n",
    "            for it, (images, voxels) in enumerate(zip(render_loader, voxel_loader)):\n",
    "                inputs=Variable(images[0])\n",
    "                labels=Variable(voxels[0])\n",
    "#                 print(\"matching: image = \", images[1], 'voxel = ',voxels[1])\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                print('inputs',inputs.shape)\n",
    "                print('labels',labels.shape)\n",
    "                \n",
    "                outputs = model(torch.stack((inputs,inputs)))\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "    \n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, labels, topk=(1,))\n",
    "            losses.update(loss.data.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            if batch_num % output_period == 0:\n",
    "                print('[%d:%.2f] loss: %.3f' % (\n",
    "                    epoch, batch_num*1.0/num_train_batches,\n",
    "                    running_loss/output_period\n",
    "                    ))\n",
    "                running_loss = 0.0\n",
    "                top1trset.append(100-top1.avg)\n",
    "                print('top1 training err (%)= '+str(top1trset[-1]))\n",
    "\n",
    "                gc.collect()\n",
    "                \n",
    "            \n",
    "\n",
    "        gc.collect()\n",
    "        # save after every epoch\n",
    "        torch.save(model.state_dict(), \"models/model.%d\" % epoch)\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        top1test = AverageMeter()\n",
    "        top5test = AverageMeter()\n",
    "        \n",
    "        for i in range((len(rand_idx)-thr)//batch_size):\n",
    "            idx = val_idx[i*batch_size: (i+1)*batch_size]\n",
    "            print(idx)\n",
    "            render_loader, voxel_loader = dataset.get_train_data_loaders(idx)\n",
    "            for it, (images, voxels) in enumerate(zip(render_loader, voxel_loader)):\n",
    "                inputs=Variable(images[0])\n",
    "                labels=Variable(voxels[0])\n",
    "#                 print(\"matching: image = \", images[1], 'voxel = ',voxels[1])\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "            \n",
    "    \n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(outputs.data, labels, topk=(1,))\n",
    "            losses.update(loss.data.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "            \n",
    "        \n",
    "        top1set.append(100-top1test.avg)\n",
    "        print('top1 val err (%)= '+str(top1set[-1]))\n",
    "        \n",
    "        gc.collect()\n",
    "        epoch += 1\n",
    "    return top1trset,top1set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "36 * 128 * 4 * 4 * 4 / 32768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "\n",
      "Your Model is \"ResidualGRUNet\" Initializing\n",
      "\n",
      "Initializing \"Encoder\"\n",
      "\n",
      "Initializing \"Decoder\"\n",
      "Current learning rate: 0.01\n",
      "[27866 27261 30875 22757]\n",
      "inputs torch.Size([4, 3, 137, 137])\n",
      "labels torch.Size([4, 32, 32, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: size '[36 x 128 x 4 x 4 x 4]' is invalid for input with 32768 elements at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/TH/THStorage.cpp:84",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c1f59febf2e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mBASE_LR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtop1trset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtop1set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training terminated'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-93f879629b51>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Pytorch-3dr2n2/models/base_gru_net.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, test)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \"\"\"\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mgru_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_gate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgru_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Pytorch-3dr2n2/models/res_gru_net.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, h, u, time)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mrect7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mt_x_s_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_x_s_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrect7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0mt_x_s_reset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_x_s_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrect7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Pytorch-3dr2n2/lib/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, fc7, h, time)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m#fc7 is the leakyReLU-ed ouput of fc7 layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m#h is the hidden state of the previous time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mfc7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mbn_fc7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m#the input of Recurrent_BatchNorm3d is (input_, time)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 2: size '[36 x 128 x 4 x 4 x 4]' is invalid for input with 32768 elements at /opt/conda/conda-bld/pytorch_1535491974311/work/aten/src/TH/THStorage.cpp:84"
     ]
    }
   ],
   "source": [
    "print('Starting training')\n",
    "\n",
    "BASE_LR = 0.001\n",
    "top1trset,top1set = run()\n",
    "print('Training terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAM, w.o. WD=0.001, 2 fc = 3, dp = 0.2, BASE_LR = 0.001\n",
    "top1trset,top5trset,top1set,top5set #~43 epoch 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAM, w.o. WD=0.001, 2 fc, dp = 0.2, BASE_LR = 0.01\n",
    "top1trset,top5trset,top1set,top5set #~43 epoch 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Starting training')\n",
    "BASE_LR = 0.01\n",
    "top1trset1,top5trset1,top1set1,top5set1 = run(0.2)\n",
    "top1trset2,top5trset2,top1set2,top5set2 = run(0.5)\n",
    "top1trset3,top5trset3,top1set3,top5set3 = run(0.8)\n",
    "print('Training terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Starting training')\n",
    "BASE_LR = 0.001\n",
    "top1trset0001,top5trset0001,top1set0001,top5set0001 = run(0)\n",
    "BASE_LR = 0.0001\n",
    "top1trset00001,top5trset00001,top1set00001,top5set00001 = run(0)\n",
    "print('Training terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import the pre-trained model\n",
    "model = resnet_18()\n",
    "model_path = 'models/trained/model.25'\n",
    "checkpoint = torch.load(model_path, map_location='cpu')\n",
    "model.load_state_dict(checkpoint)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)#.half()\n",
    "\n",
    "\n",
    "# write top 5 into result.txt\n",
    "def test_accu(batch_size):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    val_loader,test_loader = dataset.get_val_test_loaders(batch_size)\n",
    "    top1_count,top5_count = 0.0,0.0\n",
    "    print('number of test set: ' + str(len(test_loader)))\n",
    "    \n",
    "    result = open(\"result.txt\",\"w\")\n",
    "    \n",
    "#     model.eval()\n",
    "        \n",
    "#     top1test = AverageMeter()\n",
    "#     top5test = AverageMeter()\n",
    "#     for i, data in enumerate(val_loader):\n",
    "#         img, labels = data\n",
    "#         x = Variable(img)\n",
    "#         y = Variable(labels)\n",
    "\n",
    "#         x = x.to(device)#.half()\n",
    "#         y = y.to(device)\n",
    "#         outs = model(x)\n",
    "#         #get top 5 output\n",
    "#         prec1test, prec5test = accuracy(outs.data, y, topk=(1, 5))\n",
    "# #             losses.update(loss.data.item(), inputs.size(0))\n",
    "#         top1test.update(prec1test.item(), x.size(0))\n",
    "#         top5test.update(prec5test.item(), x.size(0))\n",
    "\n",
    "#     top1set.append(100-top1test.avg)\n",
    "#     top5set.append(100-top5test.avg)\n",
    "#     print('top1 val err (%)= '+str(top1set[-1]))\n",
    "#     print('top5 val err (%)= '+str(top5set[-1]))\n",
    "\n",
    "#     gc.collect()\n",
    "#     epoch += 1\n",
    "\n",
    "    model.eval()\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        img, labels = data\n",
    "        x = Variable(img)\n",
    "        y = Variable(labels)\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()#.half()\n",
    "            y = y.cuda()\n",
    "        outs = model(x)\n",
    "        _, pred1 = torch.max(outs, -1)\n",
    "        _, pred5 = torch.topk(outs, 5)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            n = int(i*batch_size+b+1)\n",
    "            result.write('test/' + str(n).zfill(8)+'.jpg '+' '.join(str(int(e)) for e in pred5[b].tolist())+'\\n')\n",
    "    \n",
    "    # overwrite the result.txt every run\n",
    "    result.truncate()\n",
    "    result.close()\n",
    "    \n",
    "# Execute\n",
    "test_accu(25)\n",
    "print('completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = args.lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "        return losses,top1,top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "shist1 = [np.array(h) for h in top1trset]\n",
    "shist2 = [np.array(h) for h in top1set]\n",
    "# shist3 = [np.array(h) for h in top5set0001]\n",
    "# shist4 = [np.array(h) for h in top5set00001]\n",
    "num_epochs=10\n",
    "\n",
    "plt.title(\"Validation vs. Training Error\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist1)),shist1,label=\"Training\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist2)),shist2,label=\"Validation\")\n",
    "# plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist3)),shist3,label=\"lr=0.001\")\n",
    "# plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist4)),shist4,label=\"lr=0.0001\")\n",
    "# plt.ylim((0,1.))\n",
    "# plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "shist1 = [np.array(h) for h in top5set001]\n",
    "shist2 = [np.array(h) for h in top5set1]\n",
    "shist3 = [np.array(h) for h in top5set2]\n",
    "shist4 = [np.array(h) for h in top5set3]\n",
    "num_epochs=10\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Dropout rate\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Error\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist1)),shist1,label=\"dropout=0\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist2)),shist2,label=\"dropout=0.2\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist3)),shist3,label=\"dropout=0.5\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist4)),shist4,label=\"dropout=0.8\")\n",
    "# plt.ylim((0,1.))\n",
    "# plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "shist1 = [np.array(h) for h in top5trset001]\n",
    "shist2 = [np.array(h) for h in top5trset1]\n",
    "shist3 = [np.array(h) for h in top5trset2]\n",
    "shist4 = [np.array(h) for h in top5trset3]\n",
    "num_epochs=10\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Dropout rate\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Training Error\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist1)),shist1,label=\"dropout=0\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist2)),shist2,label=\"dropout=0.2\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist3)),shist3,label=\"dropout=0.5\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist4)),shist4,label=\"dropout=0.8\")\n",
    "# plt.ylim((0,1.))\n",
    "# plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "shist1 = [np.array(h) for h in top5trset0]\n",
    "shist2 = [np.array(h) for h in top5trset001]\n",
    "shist3 = [np.array(h) for h in top5trset0001]\n",
    "shist4 = [np.array(h) for h in top5trset00001]\n",
    "num_epochs=10\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Learning rate\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist1)),shist1,label=\"lr=0.1\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist2)),shist2,label=\"lr=0.01\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist3)),shist3,label=\"lr=0.001\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist4)),shist4,label=\"lr=0.0001\")\n",
    "# plt.ylim((0,1.))\n",
    "# plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "range(1,10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top1trset,top5trset,top1set,top5set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        target = target.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1, top5=top5))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "\n",
    "    return losses,top1,top5\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename + '_latest.pth.tar')\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename + '_latest.pth.tar', filename + '_best.pth.tar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch36]",
   "language": "python",
   "name": "conda-env-torch36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

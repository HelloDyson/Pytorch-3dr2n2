{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "import gc\n",
    "import sys\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler\n",
    "torch.backends.cudnn.benchmark=True\n",
    "\n",
    "import time\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA enable:  True\n"
     ]
    }
   ],
   "source": [
    "# test CUDA available\n",
    "print('CUDA enable: ', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import dataset from ./lib/dataset.py\n",
    "import lib.dataset as dataset\n",
    "from models.__init__ import load_model\n",
    "from lib.config import cfg\n",
    "from lib.solver import Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "BASE_LR = 0.01\n",
    "EPOCH_DECAY = 10 # number of epochs after which the Learning rate is decayed exponentially.\n",
    "DECAY_WEIGHT = 0.001\n",
    "# cfg.CONST.IMG_W = 137\n",
    "# cfg.CONST.IMG_H = 137\n",
    "# cfg.CONST.N_VOX = 32\n",
    "# cfg.CONST.N_VIEWS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename + '_latest.pth.tar')\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename + '_latest.pth.tar', filename + '_best.pth.tar')\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050816 43783\n"
     ]
    }
   ],
   "source": [
    "# training hyperparameters\n",
    "batch_size = 4\n",
    "train_val_ratio = 0.8\n",
    "\n",
    "# pre setting device and data set length\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ren_len = dataset.ren_dataset.__len__()\n",
    "vox_len = dataset.vox_dataset.__len__()\n",
    "print(ren_len,vox_len)\n",
    "\n",
    "dict_ren1 = dataset.ren_dataset.class_to_idx\n",
    "list_ren = [[]]*(vox_len+1)\n",
    "\n",
    "for (path, idx) in dataset.ren_dataset.samples:\n",
    "    list_ren[idx] = list(set(list_ren[idx]))\n",
    "    list_ren[idx].append(path)\n",
    "\n",
    "\n",
    "dict_vox = {idx:path for (path, idx) in dataset.vox_dataset.samples}\n",
    "print(list_ren[202][2])\n",
    "# print(dataset.center_crop(Image.open(list_ren[202][2])))\n",
    "# print(dict_vox[202])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This function changes the learning rate over the training model.\n",
    "def exp_lr_scheduler(optimizer, epoch, init_lr=BASE_LR, lr_decay_epoch=EPOCH_DECAY):\n",
    "    \"\"\"Decay learning rate by a factor of DECAY_WEIGHT every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = init_lr * (DECAY_WEIGHT**(epoch // lr_decay_epoch))\n",
    "\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print('LR is set to {}'.format(lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your Model is \"ResidualGRUNet\" Initializing\n",
      "\n",
      "Initializing \"Encoder\"\n",
      "\n",
      "Initializing \"Decoder\"\n",
      "torch.Size([2, 4, 127, 3, 127])\n",
      "torch.Size([4, 2, 32, 32, 32])\n",
      "torch.Size([2, 4, 127, 3, 127])\n",
      "torch.Size([4, 2, 32, 32, 32])\n",
      "torch.Size([5, 4, 127, 3, 127])\n",
      "torch.Size([4, 2, 32, 32, 32])\n",
      "torch.Size([2, 4, 127, 3, 127])\n",
      "torch.Size([4, 2, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "NetClass = load_model(cfg.CONST.NETWORK_CLASS)\n",
    "# print('Network definition: \\n')\n",
    "net = NetClass()\n",
    "# print(net)\n",
    "\n",
    "# start an epoch\n",
    "# slice training and validation index\n",
    "rand_idx = np.random.permutation(np.arange(vox_len))\n",
    "thr = int(train_val_ratio*len(rand_idx))\n",
    "train_idx = rand_idx[:thr]\n",
    "val_idx = rand_idx[thr:]\n",
    "\n",
    "batch_size = 4\n",
    "max_num_views = 5\n",
    "\n",
    "dict_vox = {idx:path for (path, idx) in dataset.vox_dataset.samples}\n",
    "\n",
    "        \n",
    "for i in range(thr//batch_size):\n",
    "    \n",
    "    # for each batch\n",
    "    num_views = random.randint(2,max_num_views)\n",
    "    \n",
    "    idx = train_idx[i*batch_size: (i+1)*batch_size]\n",
    "    voxel_loader = dataset.get_vox_data_loaders(idx)\n",
    "    \n",
    "    label_list = []\n",
    "    for it, (labels, model_ids) in enumerate(voxel_loader):\n",
    "        \n",
    "        batch_image = []\n",
    "        for model_id in model_ids:\n",
    "            image_list = []\n",
    "            image_ids = np.random.choice(cfg.TRAIN.NUM_RENDERING, num_views)\n",
    "#             print(image_ids)\n",
    "            for n_view in range(num_views):\n",
    "                image_list.append(dataset.center_crop(Image.open(list_ren[(model_id).item()][image_ids[n_view]]))[:3])\n",
    "            \n",
    "            image_1 = torch.stack(image_list , dim=0)\n",
    "            batch_image.append(image_1)\n",
    "#             print(image_1.shape)\n",
    "        batch_image = torch.stack(batch_image,dim=0)\n",
    "        batch_image = batch_image.transpose(1,0)\n",
    "        batch_image = batch_image.transpose(4,2)\n",
    "        batch_image = batch_image.transpose(4,3)\n",
    "\n",
    "        labels0 = (labels < 1)        \n",
    "        batch_voxel = torch.stack((labels0.float(),labels.float()),dim=0)\n",
    "        batch_voxel = batch_voxel.transpose(1,0)        \n",
    "        \n",
    "        inputs=Variable(batch_image)\n",
    "        labels=Variable(batch_voxel)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "    print(inputs.shape)\n",
    "    print(labels.shape)\n",
    "    # test mode\n",
    "    if i ==3:\n",
    "        break\n",
    "    # test mode end            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    # Parameters\n",
    "    num_epochs = 10\n",
    "    output_period = 100\n",
    "    batch_size = 4\n",
    "    \n",
    "    # setup the device for running\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    NetClass = load_model(cfg.CONST.NETWORK_CLASS)\n",
    "    model = NetClass().to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=BASE_LR,momentum=0.9,weight_decay=DECAY_WEIGHT)\n",
    "    top1trset,top5trset = [],[]\n",
    "    top1set,top5set = [],[]\n",
    "    epoch = 1\n",
    "    while epoch <= num_epochs:\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "        top5 = AverageMeter()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        optimizer = exp_lr_scheduler(optimizer, epoch)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('Current learning rate: ' + str(param_group['lr']))\n",
    "            \n",
    "        model.train()\n",
    "\n",
    "        # start an epoch\n",
    "        # slice training and validation index\n",
    "        rand_idx = np.random.permutation(np.arange(vox_len))\n",
    "        thr = int(train_val_ratio*len(rand_idx))\n",
    "        train_idx = rand_idx[:thr]\n",
    "        val_idx = rand_idx[thr:]\n",
    "\n",
    "        batch_size = 4\n",
    "        max_num_views = 5\n",
    "\n",
    "        dict_vox = {idx:path for (path, idx) in dataset.vox_dataset.samples}\n",
    "\n",
    "        num_train_batches = thr//batch_size\n",
    "        for i in range(num_train_batches):\n",
    "\n",
    "            # for each batch\n",
    "            num_views = random.randint(2,max_num_views)\n",
    "\n",
    "            idx = train_idx[i*batch_size: (i+1)*batch_size]\n",
    "            voxel_loader = dataset.get_vox_data_loaders(idx)\n",
    "\n",
    "            label_list = []\n",
    "            for it, (labels, model_ids) in enumerate(voxel_loader):\n",
    "\n",
    "                batch_image = []\n",
    "                for model_id in model_ids:\n",
    "                    image_list = []\n",
    "                    image_ids = np.random.choice(cfg.TRAIN.NUM_RENDERING, num_views)\n",
    "        #             print(image_ids)\n",
    "                    for n_view in range(num_views):\n",
    "                        image_list.append(dataset.center_crop(Image.open(list_ren[(model_id).item()][image_ids[n_view]]))[:3])\n",
    "\n",
    "                    image_1 = torch.stack(image_list , dim=0)\n",
    "                    batch_image.append(image_1)\n",
    "        #             print(image_1.shape)\n",
    "                batch_image = torch.stack(batch_image,dim=0)\n",
    "                batch_image = batch_image.transpose(1,0)\n",
    "                batch_image = batch_image.transpose(4,2)\n",
    "                batch_image = batch_image.transpose(4,3)\n",
    "\n",
    "                labels0 = (labels < 1)        \n",
    "                batch_voxel = torch.stack((labels0.float(),labels.float()),dim=0)\n",
    "                batch_voxel = batch_voxel.transpose(1,0)        \n",
    "\n",
    "                inputs=Variable(batch_image)\n",
    "                labels=Variable(batch_voxel)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(torch.stack((inputs,inputs)))\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "    \n",
    "                # measure accuracy and record loss\n",
    "                prec1 = accuracy(outputs.data, labels, topk=(1,))\n",
    "                losses.update(loss.data.item(), inputs.size(0))\n",
    "                top1.update(prec1.item(), inputs.size(0))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "                running_loss += loss.item()\n",
    "        \n",
    "                if i % output_period == 0:\n",
    "                    print('[%d:%.2f] loss: %.3f' % (\n",
    "                        epoch, i*1.0/num_train_batches,\n",
    "                        running_loss/output_period\n",
    "                        ))\n",
    "                    running_loss = 0.0\n",
    "                    top1trset.append(100-top1.avg)\n",
    "                    print('top1 training err (%)= '+str(top1trset[-1]))\n",
    "\n",
    "                    gc.collect()\n",
    "                \n",
    "            \n",
    "\n",
    "        gc.collect()\n",
    "        # save after every epoch\n",
    "        torch.save(model.state_dict(), \"models/model.%d\" % epoch)\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        top1test = AverageMeter()\n",
    "        top5test = AverageMeter()\n",
    "        batch_size_val = 4\n",
    "        for i in range((len(rand_idx)-thr)//batch_size_val):\n",
    "            # for each batch\n",
    "            num_views = 1\n",
    "\n",
    "            idx = val_idx[i*batch_size_val: (i+1)*batch_size_val]\n",
    "            voxel_loader = dataset.get_vox_data_loaders(idx)\n",
    "\n",
    "            label_list = []\n",
    "            for it, (labels, model_ids) in enumerate(voxel_loader):\n",
    "\n",
    "                batch_image = []\n",
    "                for model_id in model_ids:\n",
    "                    image_list = []\n",
    "                    image_ids = np.random.choice(cfg.TRAIN.NUM_RENDERING, num_views)\n",
    "                    for n_view in range(num_views):\n",
    "                        image_list.append(dataset.center_crop(Image.open(list_ren[(model_id).item()][image_ids[n_view]]))[:3])\n",
    "\n",
    "                    image_1 = torch.stack(image_list , dim=0)\n",
    "                    batch_image.append(image_1)\n",
    "\n",
    "                batch_image = torch.stack(batch_image,dim=0)\n",
    "                batch_image = batch_image.transpose(1,0)\n",
    "                batch_image = batch_image.transpose(4,2)\n",
    "                batch_image = batch_image.transpose(4,3)\n",
    "\n",
    "                labels0 = (labels < 1)        \n",
    "                batch_voxel = torch.stack((labels0.float(),labels.float()),dim=0)\n",
    "                batch_voxel = batch_voxel.transpose(1,0)        \n",
    "\n",
    "                inputs=Variable(batch_image)\n",
    "                labels=Variable(batch_voxel)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "    \n",
    "                # measure accuracy and record loss\n",
    "                prec1 = accuracy(outputs.data, labels, topk=(1,))\n",
    "                losses.update(loss.data.item(), inputs.size(0))\n",
    "                top1.update(prec1.item(), inputs.size(0))\n",
    "            \n",
    "        \n",
    "        top1set.append(100-top1test.avg)\n",
    "        print('top1 val err (%)= '+str(top1set[-1]))\n",
    "        \n",
    "        gc.collect()\n",
    "        epoch += 1\n",
    "    return top1trset,top1set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "36 * 128 * 4 * 4 * 4 / 32768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Starting training')\n",
    "\n",
    "BASE_LR = 0.001\n",
    "top1trset,top1set = run()\n",
    "print('Training terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAM, w.o. WD=0.001, 2 fc = 3, dp = 0.2, BASE_LR = 0.001\n",
    "top1trset,top5trset,top1set,top5set #~43 epoch 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAM, w.o. WD=0.001, 2 fc, dp = 0.2, BASE_LR = 0.01\n",
    "top1trset,top5trset,top1set,top5set #~43 epoch 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Starting training')\n",
    "BASE_LR = 0.01\n",
    "top1trset1,top5trset1,top1set1,top5set1 = run(0.2)\n",
    "top1trset2,top5trset2,top1set2,top5set2 = run(0.5)\n",
    "top1trset3,top5trset3,top1set3,top5set3 = run(0.8)\n",
    "print('Training terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Starting training')\n",
    "BASE_LR = 0.001\n",
    "top1trset0001,top5trset0001,top1set0001,top5set0001 = run(0)\n",
    "BASE_LR = 0.0001\n",
    "top1trset00001,top5trset00001,top1set00001,top5set00001 = run(0)\n",
    "print('Training terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import the pre-trained model\n",
    "model = resnet_18()\n",
    "model_path = 'models/trained/model.25'\n",
    "checkpoint = torch.load(model_path, map_location='cpu')\n",
    "model.load_state_dict(checkpoint)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)#.half()\n",
    "\n",
    "\n",
    "# write top 5 into result.txt\n",
    "def test_accu(batch_size):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    val_loader,test_loader = dataset.get_val_test_loaders(batch_size)\n",
    "    top1_count,top5_count = 0.0,0.0\n",
    "    print('number of test set: ' + str(len(test_loader)))\n",
    "    \n",
    "    result = open(\"result.txt\",\"w\")\n",
    "    \n",
    "#     model.eval()\n",
    "        \n",
    "#     top1test = AverageMeter()\n",
    "#     top5test = AverageMeter()\n",
    "#     for i, data in enumerate(val_loader):\n",
    "#         img, labels = data\n",
    "#         x = Variable(img)\n",
    "#         y = Variable(labels)\n",
    "\n",
    "#         x = x.to(device)#.half()\n",
    "#         y = y.to(device)\n",
    "#         outs = model(x)\n",
    "#         #get top 5 output\n",
    "#         prec1test, prec5test = accuracy(outs.data, y, topk=(1, 5))\n",
    "# #             losses.update(loss.data.item(), inputs.size(0))\n",
    "#         top1test.update(prec1test.item(), x.size(0))\n",
    "#         top5test.update(prec5test.item(), x.size(0))\n",
    "\n",
    "#     top1set.append(100-top1test.avg)\n",
    "#     top5set.append(100-top5test.avg)\n",
    "#     print('top1 val err (%)= '+str(top1set[-1]))\n",
    "#     print('top5 val err (%)= '+str(top5set[-1]))\n",
    "\n",
    "#     gc.collect()\n",
    "#     epoch += 1\n",
    "\n",
    "    model.eval()\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        img, labels = data\n",
    "        x = Variable(img)\n",
    "        y = Variable(labels)\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()#.half()\n",
    "            y = y.cuda()\n",
    "        outs = model(x)\n",
    "        _, pred1 = torch.max(outs, -1)\n",
    "        _, pred5 = torch.topk(outs, 5)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            n = int(i*batch_size+b+1)\n",
    "            result.write('test/' + str(n).zfill(8)+'.jpg '+' '.join(str(int(e)) for e in pred5[b].tolist())+'\\n')\n",
    "    \n",
    "    # overwrite the result.txt every run\n",
    "    result.truncate()\n",
    "    result.close()\n",
    "    \n",
    "# Execute\n",
    "test_accu(25)\n",
    "print('completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = args.lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1, top5=top5))\n",
    "        return losses,top1,top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "shist1 = [np.array(h) for h in top1trset]\n",
    "shist2 = [np.array(h) for h in top1set]\n",
    "# shist3 = [np.array(h) for h in top5set0001]\n",
    "# shist4 = [np.array(h) for h in top5set00001]\n",
    "num_epochs=10\n",
    "\n",
    "plt.title(\"Validation vs. Training Error\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist1)),shist1,label=\"Training\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist2)),shist2,label=\"Validation\")\n",
    "# plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist3)),shist3,label=\"lr=0.001\")\n",
    "# plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist4)),shist4,label=\"lr=0.0001\")\n",
    "# plt.ylim((0,1.))\n",
    "# plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "shist1 = [np.array(h) for h in top5set001]\n",
    "shist2 = [np.array(h) for h in top5set1]\n",
    "shist3 = [np.array(h) for h in top5set2]\n",
    "shist4 = [np.array(h) for h in top5set3]\n",
    "num_epochs=10\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Dropout rate\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Error\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist1)),shist1,label=\"dropout=0\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist2)),shist2,label=\"dropout=0.2\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist3)),shist3,label=\"dropout=0.5\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist4)),shist4,label=\"dropout=0.8\")\n",
    "# plt.ylim((0,1.))\n",
    "# plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "shist1 = [np.array(h) for h in top5trset001]\n",
    "shist2 = [np.array(h) for h in top5trset1]\n",
    "shist3 = [np.array(h) for h in top5trset2]\n",
    "shist4 = [np.array(h) for h in top5trset3]\n",
    "num_epochs=10\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Dropout rate\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Training Error\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist1)),shist1,label=\"dropout=0\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist2)),shist2,label=\"dropout=0.2\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist3)),shist3,label=\"dropout=0.5\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist4)),shist4,label=\"dropout=0.8\")\n",
    "# plt.ylim((0,1.))\n",
    "# plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "shist1 = [np.array(h) for h in top5trset0]\n",
    "shist2 = [np.array(h) for h in top5trset001]\n",
    "shist3 = [np.array(h) for h in top5trset0001]\n",
    "shist4 = [np.array(h) for h in top5trset00001]\n",
    "num_epochs=10\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Learning rate\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist1)),shist1,label=\"lr=0.1\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist2)),shist2,label=\"lr=0.01\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist3)),shist3,label=\"lr=0.001\")\n",
    "plt.plot(np.arange(1,num_epochs+1,num_epochs/len(shist4)),shist4,label=\"lr=0.0001\")\n",
    "# plt.ylim((0,1.))\n",
    "# plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "range(1,10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top1trset,top5trset,top1set,top5set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        target = target.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1, top5=top5))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "\n",
    "    return losses,top1,top5\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename + '_latest.pth.tar')\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename + '_latest.pth.tar', filename + '_best.pth.tar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch36]",
   "language": "python",
   "name": "conda-env-torch36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
